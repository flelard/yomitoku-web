import os
import uuid
import subprocess
import requests
import time
import json
import threading
from collections import deque
from pathlib import Path
from flask import Flask, render_template, request, send_file, jsonify, session, redirect, url_for, Response
from werkzeug.utils import secure_filename
from concurrent.futures import ThreadPoolExecutor
import atexit

app = Flask(__name__)
app.config['SECRET_KEY'] = 'cle-multilingue-yomitoku-ollama-2024'
app.config['UPLOAD_FOLDER'] = 'static/uploads'
app.config['OUTPUT_FOLDER'] = 'output'
app.config['MAX_CONTENT_LENGTH'] = 50 * 1024 * 1024
app.config['OLLAMA_TIMEOUT'] = 300
app.config['OLLAMA_MODEL'] = 'qwen3:8b'

os.makedirs(app.config['UPLOAD_FOLDER'], exist_ok=True)
os.makedirs(app.config['OUTPUT_FOLDER'], exist_ok=True)

AVAILABLE_OLLAMA_MODELS = []

# Thread pool pour exÃ©cuter les jobs en arriÃ¨re-plan
executor = ThreadPoolExecutor(max_workers=3)
atexit.register(lambda: executor.shutdown(wait=True))

# Stockage des donnÃ©es de job
job_data = {}
data_lock = threading.Lock()

def log_to_job(job_id, message, level='info', progress=None):
    """Ajoute un log et/ou une progression au job avec thread safety"""
    with data_lock:
        if job_id not in job_data:
            job_data[job_id] = {
                'logs': deque(maxlen=1000),
                'progress': 0,
                'status': 'running',
                'current_page': None,
                'total_pages': None
            }
        
        job_data[job_id]['logs'].append({
            'timestamp': time.time(),
            'message': message,
            'level': level
        })
        
        if progress is not None:
            job_data[job_id]['progress'] = progress
        
        # DÃ©tecter la progression dans le message
        if 'Processing page' in message:
            parts = message.split()
            try:
                current = int(parts[2].split('/')[0])
                total = int(parts[2].split('/')[1])
                job_data[job_id]['progress'] = (current / total) * 100
                job_data[job_id]['current_page'] = current
                job_data[job_id]['total_pages'] = total
            except:
                pass
    
    print(f"[{job_id}] {message}")

def detect_ollama_models():
    """DÃ©tecte les modÃ¨les Ollama disponibles au dÃ©marrage"""
    global AVAILABLE_OLLAMA_MODELS
    print(f"\n{'='*60}")
    print("ğŸ” DÃ‰TECTION DES MODÃˆLES OLLAMA...")
    
    try:
        response = requests.get('http://127.0.0.1:11434/api/tags', timeout=5)
        if response.status_code == 200:
            models = response.json().get('models', [])
            AVAILABLE_OLLAMA_MODELS = [m['name'] for m in models]
            print(f"âœ… {len(AVAILABLE_OLLAMA_MODELS)} modÃ¨les dÃ©tectÃ©s:")
            for model in AVAILABLE_OLLAMA_MODELS:
                print(f"   ğŸ“¦ {model}")
            print(f"{'='*60}\n")
            return True
        else:
            print(f"âŒ Erreur Ollama: {response.status_code}")
            return False
    except requests.exceptions.ConnectionError:
        print("âŒ Ollama n'est pas accessible sur 127.0.0.1:11434")
        print("ğŸ’¡ DÃ©marrez Ollama avec: sudo systemctl start ollama")
        print(f"{'='*60}\n")
        return False
    except Exception as e:
        print(f"âŒ Erreur: {e}")
        print(f"{'='*60}\n")
        return False

def cleanup_gpu_memory(job_id=None):
    """LibÃ¨re la mÃ©moire GPU (CUDA) pour Ã©viter les OutOfMemory"""
    try:
        import torch
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
            
            # Obtenir les stats mÃ©moire
            allocated = torch.cuda.memory_allocated() / 1024**3  # GB
            reserved = torch.cuda.memory_reserved() / 1024**3    # GB
            
            if job_id:
                log_to_job(job_id, f"ğŸ§¹ GPU nettoyÃ© - AllouÃ©: {allocated:.2f}GB, RÃ©servÃ©: {reserved:.2f}GB", 'info')
            else:
                print(f"ğŸ§¹ GPU nettoyÃ© - AllouÃ©: {allocated:.2f}GB, RÃ©servÃ©: {reserved:.2f}GB")
            
            return True
    except ImportError:
        if job_id:
            log_to_job(job_id, "â„¹ï¸ PyTorch non disponible, skip nettoyage GPU", 'info')
        return False
    except Exception as e:
        if job_id:
            log_to_job(job_id, f"âš ï¸ Erreur nettoyage GPU: {e}", 'warning')
        else:
            print(f"âš ï¸ Erreur nettoyage GPU: {e}")
        return False

# DÃ©tection des modÃ¨les au dÃ©marrage
print(f"ğŸš€ DÃ‰MARRAGE SERVEUR YOMITOKU + OLLAMA")
detect_ollama_models()

# Traductions complÃ¨tes
TRANSLATIONS = {
    'fr': {
        'title': 'Yomitoku + Ollama',
        'subtitle': 'Analyse & Traduction de documents',
        'select_file': 'SÃ©lectionnez un document',
        'format_label': 'Format de sortie',
        'formats': {
            'md': 'Markdown',
            'html': 'HTML',
            'json': 'JSON',
            'csv': 'CSV'
        },
        'device': 'PÃ©riphÃ©rique',
        'options': 'Options avancÃ©es',
        'trans_options': 'Options de traduction',
        'vis': 'GÃ©nÃ©rer la visualisation',
        'lite': 'Mode lÃ©ger (rapide)',
        'figure': 'Exporter les figures',
        'figure_letter': 'Texte dans les figures',
        'ignore_line_break': 'Ignorer sauts de ligne',
        'combine': 'Fusionner les pages',
        'ignore_meta': 'Ignorer en-tÃªtes/pieds',
        'translate': 'Traduire avec Ollama',
        'target_lang': 'Langue cible',
        'target_langs': {
            'fr': 'FranÃ§ais',
            'en': 'Anglais',
            'es': 'Espagnol',
            'de': 'Allemand'
        },
        'ollama_model': 'ModÃ¨le Ollama',
        'custom_prompt': 'Prompt personnalisÃ© (optionnel)',
        'custom_prompt_help': 'Laissez vide pour utiliser le prompt par dÃ©faut. Variables disponibles: {text}, {target_lang}',
        'launch': 'Lancer l\'analyse',
        'drag_drop': 'Glissez-dÃ©posez votre fichier ici ou cliquez pour sÃ©lectionner',
        'success': 'Analyse terminÃ©e !',
        'translating': 'Traduction en cours...',
        'error': 'Erreur',
        'error_ollama': 'Erreur Ollama (non dÃ©marrÃ©?)',
        'download': 'TÃ©lÃ©charger',
        'view_results': 'Voir les rÃ©sultats',
        'job_id': 'Job ID',
        'files_generated': 'Fichiers gÃ©nÃ©rÃ©s',
        'visualizations': 'Visualisations',
        'translated_files': 'Fichiers traduits',
        'no_files': 'Aucun fichier trouvÃ©',
        'back': 'Retour',
        'recent_jobs': 'Analyses rÃ©centes',
        'no_models': 'Aucun modÃ¨le Ollama dÃ©tectÃ©',
        'refresh_models': 'Actualiser les modÃ¨les',
        # Tooltips
        'tooltip_vis': 'GÃ©nÃ¨re une image avec les zones de texte dÃ©tectÃ©es encadrÃ©es',
        'tooltip_lite': 'Utilise un modÃ¨le plus rapide mais moins prÃ©cis pour l\'OCR',
        'tooltip_figure': 'Extrait les graphiques et images du document en fichiers sÃ©parÃ©s',
        'tooltip_figure_letter': 'DÃ©tecte et extrait le texte prÃ©sent Ã  l\'intÃ©rieur des figures',
        'tooltip_ignore_line_break': 'Supprime les sauts de ligne pour crÃ©er un texte continu',
        'tooltip_combine': 'Combine toutes les pages en un seul fichier de rÃ©sultat',
        'tooltip_ignore_meta': 'Ignore les en-tÃªtes, pieds de page et numÃ©ros de page',
        'tooltip_translate': 'Active la traduction automatique via Ollama aprÃ¨s l\'OCR',
        # Progression
        'progress_processing': 'Traitement en cours...',
        'progress_page': 'Page',
        'progress_of': 'sur',
        'progress_complete': 'Analyse terminÃ©e !'
    },
    'en': {
        'title': 'Yomitoku + Ollama',
        'subtitle': 'Document Analysis & Translation',
        'select_file': 'Select a document',
        'format_label': 'Output Format',
        'formats': {
            'md': 'Markdown',
            'html': 'HTML',
            'json': 'JSON',
            'csv': 'CSV'
        },
        'device': 'Device',
        'options': 'Advanced Options',
        'trans_options': 'Translation Options',
        'vis': 'Generate visualization',
        'lite': 'Lite mode (fast)',
        'figure': 'Export figures',
        'figure_letter': 'Text in figures',
        'ignore_line_break': 'Ignore line breaks',
        'combine': 'Merge pages',
        'ignore_meta': 'Ignore headers/footers',
        'translate': 'Translate with Ollama',
        'target_lang': 'Target language',
        'target_langs': {
            'fr': 'French',
            'en': 'English',
            'es': 'Spanish',
            'de': 'German'
        },
        'ollama_model': 'Ollama Model',
        'custom_prompt': 'Custom prompt (optional)',
        'custom_prompt_help': 'Leave empty for default prompt. Available variables: {text}, {target_lang}',
        'launch': 'Launch Analysis',
        'drag_drop': 'Drag & drop your file here or click to select',
        'success': 'Analysis completed!',
        'translating': 'Translation in progress...',
        'error': 'Error',
        'error_ollama': 'Ollama error (not started?)',
        'download': 'Download',
        'view_results': 'View Results',
        'job_id': 'Job ID',
        'files_generated': 'Generated Files',
        'visualizations': 'Visualizations',
        'translated_files': 'Translated Files',
        'no_files': 'No files found',
        'back': 'Back',
        'recent_jobs': 'Recent Analyses',
        'no_models': 'No Ollama models detected',
        'refresh_models': 'Refresh models',
        # Tooltips
        'tooltip_vis': 'Generates an image with detected text areas framed',
        'tooltip_lite': 'Uses a faster but less accurate model for OCR',
        'tooltip_figure': 'Extracts charts and images from the document as separate files',
        'tooltip_figure_letter': 'Detects and extracts text inside figures and charts',
        'tooltip_ignore_line_break': 'Removes line breaks to create continuous text',
        'tooltip_combine': 'Combines all pages into a single result file',
        'tooltip_ignore_meta': 'Ignores headers, footers and page numbers',
        'tooltip_translate': 'Enables automatic translation via Ollama after OCR',
        # Progression
        'progress_processing': 'Processing...',
        'progress_page': 'Page',
        'progress_of': 'of',
        'progress_complete': 'Analysis completed!'
    },
    'ja': {
        'title': 'Yomitoku + Ollama',
        'subtitle': 'æ–‡æ›¸åˆ†æ & ç¿»è¨³',
        'select_file': 'æ–‡æ›¸ã‚’é¸æŠ',
        'format_label': 'å‡ºåŠ›å½¢å¼',
        'formats': {
            'md': 'Markdown',
            'html': 'HTML',
            'json': 'JSON',
            'csv': 'CSV'
        },
        'device': 'ãƒ‡ãƒã‚¤ã‚¹',
        'options': 'é«˜åº¦ãªã‚ªãƒ—ã‚·ãƒ§ãƒ³',
        'trans_options': 'ç¿»è¨³ã‚ªãƒ—ã‚·ãƒ§ãƒ³',
        'vis': 'å¯è¦–åŒ–ã‚’ç”Ÿæˆ',
        'lite': 'ãƒ©ã‚¤ãƒˆãƒ¢ãƒ¼ãƒ‰(é«˜é€Ÿ)',
        'figure': 'å›³ã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ',
        'figure_letter': 'å›³å†…ã®æ–‡å­—',
        'ignore_line_break': 'æ”¹è¡Œã‚’ç„¡è¦–',
        'combine': 'ãƒšãƒ¼ã‚¸ã‚’çµåˆ',
        'ignore_meta': 'ãƒ˜ãƒƒãƒ€ãƒ¼/ãƒ•ãƒƒã‚¿ãƒ¼ã‚’ç„¡è¦–',
        'translate': 'Ollamaã§ç¿»è¨³',
        'target_lang': 'ã‚¿ãƒ¼ã‚²ãƒƒãƒˆè¨€èª',
        'target_langs': {
            'fr': 'ãƒ•ãƒ©ãƒ³ã‚¹èª',
            'en': 'è‹±èª',
            'es': 'ã‚¹ãƒšã‚¤ãƒ³èª',
            'de': 'ãƒ‰ã‚¤ãƒ„èª'
        },
        'ollama_model': 'Ollamaãƒ¢ãƒ‡ãƒ«',
        'custom_prompt': 'ã‚«ã‚¹ã‚¿ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ(ã‚ªãƒ—ã‚·ãƒ§ãƒ³)',
        'custom_prompt_help': 'ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã¯ç©ºç™½ã®ã¾ã¾ã«ã—ã¾ã™ã€‚åˆ©ç”¨å¯èƒ½ãªå¤‰æ•°: {text}, {target_lang}',
        'launch': 'åˆ†æã‚’é–‹å§‹',
        'drag_drop': 'ã“ã“ã«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ‰ãƒ©ãƒƒã‚°&ãƒ‰ãƒ­ãƒƒãƒ—ã€ã¾ãŸã¯ã‚¯ãƒªãƒƒã‚¯ã—ã¦é¸æŠ',
        'success': 'åˆ†æãŒå®Œäº†ã—ã¾ã—ãŸ!',
        'translating': 'ç¿»è¨³ä¸­...',
        'error': 'ã‚¨ãƒ©ãƒ¼',
        'error_ollama': 'Ollamaã‚¨ãƒ©ãƒ¼(èµ·å‹•ã—ã¦ã„ã¾ã›ã‚“?)',
        'download': 'ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰',
        'view_results': 'çµæœã‚’è¡¨ç¤º',
        'job_id': 'ã‚¸ãƒ§ãƒ–ID',
        'files_generated': 'ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«',
        'visualizations': 'å¯è¦–åŒ–',
        'translated_files': 'ç¿»è¨³æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«',
        'no_files': 'ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“',
        'back': 'æˆ»ã‚‹',
        'recent_jobs': 'æœ€è¿‘ã®åˆ†æ',
        'no_models': 'Ollamaãƒ¢ãƒ‡ãƒ«ãŒæ¤œå‡ºã•ã‚Œã¾ã›ã‚“',
        'refresh_models': 'ãƒ¢ãƒ‡ãƒ«ã‚’æ›´æ–°',
        # Tooltips
        'tooltip_vis': 'æ¤œå‡ºã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒªã‚¢ã‚’ãƒ•ãƒ¬ãƒ¼ãƒ ã§å›²ã‚“ã ç”»åƒã‚’ç”Ÿæˆã—ã¾ã™',
        'tooltip_lite': 'OCRç”¨ã«ã‚ˆã‚Šé«˜é€Ÿã§ã™ãŒç²¾åº¦ã®ä½ã„ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¾ã™',
        'tooltip_figure': 'ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‹ã‚‰ã‚°ãƒ©ãƒ•ã‚„ç”»åƒã‚’åˆ¥ãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦æŠ½å‡ºã—ã¾ã™',
        'tooltip_figure_letter': 'å›³ã‚„ã‚°ãƒ©ãƒ•å†…ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’æ¤œå‡ºã—ã¦æŠ½å‡ºã—ã¾ã™',
        'tooltip_ignore_line_break': 'æ”¹è¡Œã‚’å‰Šé™¤ã—ã¦é€£ç¶šãƒ†ã‚­ã‚¹ãƒˆã‚’ä½œæˆã—ã¾ã™',
        'tooltip_combine': 'ã™ã¹ã¦ã®ãƒšãƒ¼ã‚¸ã‚’1ã¤ã®çµæœãƒ•ã‚¡ã‚¤ãƒ«ã«çµåˆã—ã¾ã™',
        'tooltip_ignore_meta': 'ãƒ˜ãƒƒãƒ€ãƒ¼ã€ãƒ•ãƒƒã‚¿ãƒ¼ã€ãƒšãƒ¼ã‚¸ç•ªå·ã‚’ç„¡è¦–ã—ã¾ã™',
        'tooltip_translate': 'OCRå¾Œã«Ollamaã«ã‚ˆã‚‹è‡ªå‹•ç¿»è¨³ã‚’æœ‰åŠ¹ã«ã—ã¾ã™',
        # Progression
        'progress_processing': 'å‡¦ç†ä¸­...',
        'progress_page': 'ãƒšãƒ¼ã‚¸',
        'progress_of': 'ï¼',
        'progress_complete': 'åˆ†æãŒå®Œäº†ã—ã¾ã—ãŸ!'
    }
}

ALLOWED_EXTENSIONS = {'png', 'jpg', 'jpeg', 'pdf', 'tiff', 'bmp'}

def allowed_file(filename):
    return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS

def get_job_path(job_id):
    return Path(app.config['OUTPUT_FOLDER']) / job_id

def get_lang():
    return session.get('lang', 'fr')

def translate_with_ollama(text, target_lang='fr', model=None, custom_prompt=None, job_id=None):
    """Traduit le texte avec Ollama local"""
    
    # âœ… Mapper les codes de langue vers les noms complets
    LANG_NAMES = {
        'fr': 'French',
        'en': 'English',
        'es': 'Spanish',
        'de': 'German',
        'it': 'Italian',
        'pt': 'Portuguese',
        'nl': 'Dutch',
        'ru': 'Russian',
        'zh': 'Chinese',
        'ja': 'Japanese',
        'ko': 'Korean'
    }
    
    target_lang_full = LANG_NAMES.get(target_lang, target_lang)
    
    log_to_job(job_id, f"ğŸ“„ DÃ©but traduction vers {target_lang_full}", 'info')
    log_to_job(job_id, f"ğŸ”§ ModÃ¨le: {model or app.config['OLLAMA_MODEL']}", 'info')
    
    if len(text.strip()) < 50:
        log_to_job(job_id, "âš ï¸ Texte trop court pour traduction (< 50 car)", 'warning')
        return text
    
    jap_chars = sum(1 for c in text if '\u3040' <= c <= '\u30FF' or '\u4E00' <= c <= '\u9FFF')
    log_to_job(job_id, f"ğŸ” CaractÃ¨res japonais dÃ©tectÃ©s: {jap_chars}", 'info')
    
    if jap_chars < 50:
        log_to_job(job_id, "âš ï¸ Peu de caractÃ¨res japonais, traduction annulÃ©e", 'warning')
        return text + "\n\n[âš ï¸ Texte non-japonais dÃ©tectÃ©, traduction ignorÃ©e]"
    
    try:
        model_to_use = model or app.config['OLLAMA_MODEL']
        
        # âœ… CORRECTION : Utiliser le prompt personnalisÃ© s'il existe
        if custom_prompt and custom_prompt.strip():
            # Remplacer les variables dans le prompt personnalisÃ© avec le nom complet
            final_prompt = custom_prompt.replace('{text}', text).replace('{target_lang}', target_lang_full)
            log_to_job(job_id, "âœ¨ Utilisation du prompt personnalisÃ©", 'info')
        else:
            # Prompt par dÃ©faut si aucun prompt personnalisÃ©
            final_prompt = f"Translate this Japanese text to {target_lang_full}. Return ONLY the translation:\n\n{text}"
            log_to_job(job_id, "ğŸ“ Utilisation du prompt par dÃ©faut", 'info')
        
        # Log du prompt utilisÃ© (tronquÃ© pour lisibilitÃ©)
        prompt_preview = final_prompt[:200] + "..." if len(final_prompt) > 200 else final_prompt
        log_to_job(job_id, f"ğŸ’¬ Prompt: {prompt_preview}", 'info')
        
        response = requests.post(
            'http://127.0.0.1:11434/api/generate',
            json={
                "model": model_to_use,
                "prompt": final_prompt,  # âœ… Utiliser le prompt prÃ©parÃ©
                "stream": False,
                "options": {"temperature": 0.1, "top_p": 0.9, "num_predict": 8000}
            },
            timeout=app.config['OLLAMA_TIMEOUT']
        )
        
        if response.status_code == 200:
            translated = response.json()['response'].strip()
            log_to_job(job_id, f"âœ… Traduction terminÃ©e ({len(translated)} car)", 'success')
            
            # âœ… LibÃ©rer la VRAM GPU aprÃ¨s traduction Ollama
            try:
                import torch
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                    torch.cuda.synchronize()
                    log_to_job(job_id, "ğŸ§¹ VRAM GPU libÃ©rÃ©e", 'info')
            except Exception as cleanup_error:
                log_to_job(job_id, f"âš ï¸ Nettoyage GPU Ã©chouÃ©: {cleanup_error}", 'warning')
            
            return translated
        else:
            log_to_job(job_id, f"âŒ Erreur Ollama: {response.status_code}", 'error')
            return f"âŒ Translation failed: {response.status_code}"
            
    except requests.exceptions.ConnectionError:
        log_to_job(job_id, "âŒ ERREUR: Ollama non accessible", 'error')
        return "âŒ ERROR: Ollama not running"
    except Exception as e:
        log_to_job(job_id, f"âŒ Exception: {type(e).__name__}: {str(e)}", 'error')
        return f"âŒ Translation error: {str(e)}"

def run_yomitoku_job(job_id, input_path, cmd, translate_enabled, target_lang, ollama_model, custom_prompt, job_path):
    """ExÃ©cute Yomitoku et la traduction en arriÃ¨re-plan"""
    try:
        log_to_job(job_id, f"ğŸ“„ NOUVELLE ANALYSE - Job ID: {job_id}", 'info')
        log_to_job(job_id, f"ğŸ“ Fichier: {input_path.name} ({input_path.stat().st_size} bytes)", 'info')
        log_to_job(job_id, f"ğŸ”§ Commande: {' '.join(cmd)}", 'info')
        
        # Nettoyage GPU avant Yomitoku
        cleanup_gpu_memory(job_id)
        
        # DÃ©marrer Yomitoku
        log_to_job(job_id, "â³ DÃ©marrage d'Yomitoku...", 'info')
        
        process = subprocess.Popen(
            cmd,
            stdout=subprocess.PIPE,
            stderr=subprocess.STDOUT,
            universal_newlines=True,
            bufsize=1
        )
        
        # Lire la sortie EN TEMPS RÃ‰EL
        for line in iter(process.stdout.readline, ''):
            if line:
                line = line.strip()
                
                # Capturer la progression
                if 'Processing page' in line:
                    parts = line.split()
                    try:
                        current = int(parts[2].split('/')[0])
                        total = int(parts[2].split('/')[1])
                        progress = (current / total) * 100
                        log_to_job(job_id, line, 'info', progress)
                    except:
                        log_to_job(job_id, line, 'info')
                else:
                    log_to_job(job_id, line, 'info')
        
        returncode = process.wait()
        
        # Nettoyage GPU aprÃ¨s Yomitoku
        cleanup_gpu_memory(job_id)
        
        if returncode != 0:
            log_to_job(job_id, f"âŒ Erreur Yomitoku (code {returncode})", 'error')
            with data_lock:
                job_data[job_id]['status'] = 'error'
            return
        
        log_to_job(job_id, "âœ… Analyse Yomitoku terminÃ©e", 'success')
        
        # TRADUCTION
        if translate_enabled:
            log_to_job(job_id, f"\nğŸŒ TRADUCTION vers {target_lang} avec {ollama_model}", 'info')
            
            # Nettoyage GPU avant traduction
            cleanup_gpu_memory(job_id)
            
            results_dir = job_path / 'results'
            
            files_to_translate = []
            for ext in ['*.md', '*.html', '*.txt', '*.json', '*.csv']:
                files_to_translate.extend(results_dir.glob(ext))
            
            log_to_job(job_id, f"ğŸ“„ Fichiers Ã  traduire: {len(files_to_translate)}", 'info')
            
            if not files_to_translate:
                log_to_job(job_id, "âŒ Aucun fichier texte trouvÃ© !", 'error')
            
            for i, file_path in enumerate(files_to_translate[:2]):
                log_to_job(job_id, f"\nğŸ“ Traduction fichier {i+1}/{len(files_to_translate)}: {file_path.name}", 'info')
                
                try:
                    text = file_path.read_text(encoding='utf-8', errors='ignore')
                    log_to_job(job_id, f"ğŸ“Š Taille texte: {len(text)} caractÃ¨res", 'info')
                    
                    translated = translate_with_ollama(text, target_lang, ollama_model, custom_prompt, job_id)
                    
                    if translated and not translated.startswith('âŒ'):
                        translated_file = results_dir / f"translated_{target_lang}_{file_path.name}"
                        translated_file.write_text(translated, encoding='utf-8')
                        log_to_job(job_id, f"âœ… SauvegardÃ©: {translated_file.name}", 'success')
                    else:
                        log_to_job(job_id, f"âŒ Ã‰chec: {translated}", 'error')
                        
                except Exception as e:
                    log_to_job(job_id, f"âŒ Exception: {e}", 'error')
                
                # Nettoyage GPU aprÃ¨s chaque fichier
                cleanup_gpu_memory(job_id)
        
        # Nettoyage GPU final
        cleanup_gpu_memory(job_id)
        
        # Finaliser
        with data_lock:
            job_data[job_id]['status'] = 'complete'
            job_data[job_id]['progress'] = 100
            
    except Exception as e:
        log_to_job(job_id, f"âŒ Exception gÃ©nÃ©rale: {str(e)}", 'error')
        with data_lock:
            job_data[job_id]['status'] = 'error'
        
        # Nettoyage GPU mÃªme en cas d'erreur
        cleanup_gpu_memory(job_id)

@app.route('/')
def index():
    lang = get_lang()
    return render_template('index.html', 
                         lang=lang, 
                         translations=TRANSLATIONS[lang],
                         ollama_models=AVAILABLE_OLLAMA_MODELS)

@app.route('/set_lang/<lang>')
def set_language(lang):
    if lang in ['fr', 'en', 'ja']:
        session['lang'] = lang
        print(f"ğŸŒ Changement langue: {lang}")
    return redirect(request.referrer or url_for('index'))

@app.route('/api/ollama/models')
def get_ollama_models():
    """API pour rÃ©cupÃ©rer la liste des modÃ¨les Ollama"""
    try:
        response = requests.get('http://127.0.0.1:11434/api/tags', timeout=5)
        if response.status_code == 200:
            models = response.json().get('models', [])
            model_names = [m['name'] for m in models]
            global AVAILABLE_OLLAMA_MODELS
            AVAILABLE_OLLAMA_MODELS = model_names
            print(f"âœ… API /api/ollama/models : {len(model_names)} modÃ¨les trouvÃ©s")
            
            return jsonify({
                'models': model_names,
                'count': len(model_names),
                'status': 'ok'
            })
        else:
            print(f"âŒ Ollama API erreur: {response.status_code}")
            return jsonify({
                'models': [],
                'count': 0,
                'status': 'error',
                'message': f'Ollama returned status {response.status_code}'
            }), 500
    except requests.exceptions.ConnectionError:
        print("âŒ Ollama non accessible sur 127.0.0.1:11434")
        return jsonify({
            'models': [],
            'count': 0,
            'status': 'error',
            'message': 'Ollama is not running on the server (127.0.0.1:11434)'
        }), 503
    except Exception as e:
        print(f"âŒ Exception API: {e}")
        return jsonify({
            'models': [],
            'count': 0,
            'status': 'error',
            'message': str(e)
        }), 500

@app.route('/api/logs/<job_id>')
def stream_logs(job_id):
    """Stream les logs en temps rÃ©el via Server-Sent Events"""
    def generate():
        timeout = 0
        while job_id not in job_data and timeout < 30:
            time.sleep(0.1)
            timeout += 0.1
        
        if job_id not in job_data:
            yield f"data: {json.dumps({'error': 'Job not found'})}\n\n"
            return
        
        last_log_count = 0
        last_progress = 0
        
        while True:
            with data_lock:
                data = job_data.get(job_id, {})
                logs = list(data.get('logs', []))
                progress = data.get('progress', 0)
                status = data.get('status', 'running')
                current_page = data.get('current_page')
                total_pages = data.get('total_pages')
            
            # Envoyer nouveaux logs
            if len(logs) > last_log_count:
                for log in logs[last_log_count:]:
                    yield f"data: {json.dumps({'type': 'log', 'log': log})}\n\n"
                last_log_count = len(logs)
            
            # Envoyer progression si changÃ©e
            if progress != last_progress:
                yield f"data: {json.dumps({'type': 'progress', 'progress': progress, 'current_page': current_page, 'total_pages': total_pages})}\n\n"
                last_progress = progress
            
            # VÃ©rifier fin
            if status in ['complete', 'error']:
                yield f"data: {json.dumps({'type': 'status', 'status': status})}\n\n"
                break
            
            time.sleep(0.2)
    
    return Response(generate(), mimetype='text/event-stream')

@app.route('/upload', methods=['POST'])
def upload_file():
    if 'file' not in request.files:
        return jsonify({'error': 'No file provided'}), 400
    
    file = request.files['file']
    if file.filename == '':
        return jsonify({'error': 'No file selected'}), 400
    
    if not allowed_file(file.filename):
        return jsonify({'error': 'File format not allowed'}), 400
    
    job_id = str(uuid.uuid4())[:8]
    job_path = get_job_path(job_id)
    job_path.mkdir(exist_ok=True)
    
    filename = secure_filename(file.filename)
    input_path = job_path / filename
    file.save(input_path)
    
    # Construire la commande
    output_format = request.form.get('format', 'md')
    device = request.form.get('device', 'cpu')
    translate_enabled = 'translate' in request.form
    target_lang = request.form.get('target_lang', 'fr')
    ollama_model = request.form.get('ollama_model', app.config['OLLAMA_MODEL'])
    custom_prompt = request.form.get('custom_prompt', '').strip()
    
    cmd = ['yomitoku', str(input_path), '-f', output_format, '-o', str(job_path / 'results'), '-d', device]
    
    if 'vis' in request.form: cmd.append('-v')
    if 'lite' in request.form: cmd.append('-l')
    if 'figure' in request.form: cmd.append('--figure')
    if 'figure_letter' in request.form: cmd.append('--figure_letter')
    if 'ignore_line_break' in request.form: cmd.append('--ignore_line_break')
    if 'combine' in request.form: cmd.append('--combine')
    if 'ignore_meta' in request.form: cmd.append('--ignore_meta')
    
    # DÃ‰MARRER LE TRAITEMENT EN ARRIÃˆRE-PLAN (libÃ¨re immÃ©diatement la route)
    executor.submit(run_yomitoku_job, job_id, input_path, cmd, translate_enabled, target_lang, ollama_model, custom_prompt, job_path)
    
    # RETOURNER IMMÃ‰DIATEMENT avec le job_id
    return jsonify({'job_id': job_id, 'success': True, 'files': []})

@app.route('/download/<job_id>/<filename>')
def download_file(job_id, filename):
    job_path = get_job_path(job_id)
    file_path = job_path / 'results' / filename
    
    if not file_path.exists():
        return "File not found", 404
    
    return send_file(file_path, as_attachment=True)

@app.route('/view/<job_id>/<filename>')
def view_file(job_id, filename):
    """Affiche un fichier dans le navigateur (au lieu de le tÃ©lÃ©charger)"""
    job_path = get_job_path(job_id)
    file_path = job_path / 'results' / filename
    
    if not file_path.exists():
        return "File not found", 404
    
    # DÃ©tecter le type MIME
    mime_types = {
        '.txt': 'text/plain',
        '.md': 'text/markdown',
        '.html': 'text/html',
        '.json': 'application/json',
        '.csv': 'text/csv',
        '.jpg': 'image/jpeg',
        '.jpeg': 'image/jpeg',
        '.png': 'image/png',
        '.pdf': 'application/pdf'
    }
    
    ext = file_path.suffix.lower()
    mime_type = mime_types.get(ext, 'application/octet-stream')
    
    # Pour les fichiers texte, forcer l'encodage UTF-8
    if ext in ['.txt', '.md', '.html', '.json', '.csv']:
        return send_file(file_path, mimetype=mime_type, as_attachment=False)
    
    # Pour les images et PDFs, afficher dans le navigateur
    return send_file(file_path, mimetype=mime_type, as_attachment=False)

@app.route('/results/<job_id>')
def view_results(job_id):
    lang = get_lang()
    job_path = get_job_path(job_id)
    results_dir = job_path / 'results'
    
    if not results_dir.exists():
        return "Results not found", 404
    
    files = []
    visualizations = []
    translated_files = []
    
    for file in results_dir.iterdir():
        if file.is_file():
            if file.suffix.lower() in ['.jpg', '.jpeg', '.png'] and 'vis' in file.name:
                visualizations.append(file.name)
            elif file.name.startswith('translated_'):
                translated_files.append(file.name)
            else:
                files.append({
                    'name': file.name,
                    'size': f"{file.stat().st_size / 1024:.1f} KB"
                })
    
    return render_template('results.html', job_id=job_id, files=files, visualizations=visualizations,
                         translated_files=translated_files, lang=lang, translations=TRANSLATIONS[lang])

# ===== NOUVELLES ROUTES =====

@app.route('/jobs')
def list_jobs_page():
    """Page de navigation entre tous les jobs"""
    lang = get_lang()
    return render_template('jobs.html', 
                         lang=lang, 
                         translations=TRANSLATIONS[lang])

@app.route('/api/jobs')
def list_jobs():
    """API amÃ©liorÃ©e pour lister les jobs avec plus d'infos"""
    jobs = []
    output_path = Path(app.config['OUTPUT_FOLDER'])
    
    for job_dir in output_path.iterdir():
        if job_dir.is_dir() and (job_dir / 'results').exists():
            created = job_dir.stat().st_ctime
            files_count = len(list((job_dir / 'results').glob('*')))
            
            jobs.append({
                'id': job_dir.name,
                'created': created,
                'files_count': files_count,
                'has_visualizations': len(list((job_dir / 'results').glob('*.png'))) > 0,
                'has_translations': len(list((job_dir / 'results').glob('translated_*'))) > 0
            })
    
    jobs.sort(key=lambda x: x['created'], reverse=True)
    return jsonify(jobs)

@app.route('/api/job/<job_id>')
def get_job_info(job_id):
    """Obtenir les dÃ©tails d'un job spÃ©cifique"""
    job_path = get_job_path(job_id)
    if not job_path.exists():
        return jsonify({'error': 'Job not found'}), 404
    
    results_dir = job_path / 'results'
    if not results_dir.exists():
        return jsonify({'error': 'No results found'}), 404
    
    files = []
    visualizations = []
    translated_files = []
    
    for file in results_dir.iterdir():
        if file.is_file():
            file_info = {
                'name': file.name,
                'size': file.stat().st_size,
                'url': url_for('download_file', job_id=job_id, filename=file.name),
                'view_url': url_for('view_file', job_id=job_id, filename=file.name)
            }
            
            if file.suffix.lower() in ['.jpg', '.jpeg', '.png'] and 'vis' in file.name:
                visualizations.append(file_info)
            elif file.name.startswith('translated_'):
                translated_files.append(file_info)
            else:
                files.append(file_info)
    
    return jsonify({
        'job_id': job_id,
        'files': files,
        'visualizations': visualizations,
        'translated_files': translated_files,
        'created': job_path.stat().st_ctime
    })

if __name__ == '__main__':
    print("ğŸš€ DÃ‰MARRAGE DU SERVEUR...")
    print(f"ğŸŒ AccÃ©dez Ã  : http://<IP_VOTRE_SERVEUR>:5000")
    if AVAILABLE_OLLAMA_MODELS:
        print(f"ğŸ“¦ {len(AVAILABLE_OLLAMA_MODELS)} modÃ¨les Ollama dÃ©tectÃ©s")
    else:
        print("âš ï¸  Aucun modÃ¨le Ollama dÃ©tectÃ©")
    app.run(debug=False, host='0.0.0.0', port=5000)
